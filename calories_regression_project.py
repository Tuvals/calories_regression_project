# -*- coding: utf-8 -*-
"""Calories Regression Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/128onXIOcg27LvyShbvQxlGAx3ydaUJr3

# Importing
"""

# Commented out IPython magic to ensure Python compatibility.
# Ana
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sb
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split as split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.neighbors import KNeighborsRegressor

from sklearn.metrics import mean_squared_log_error as msle
# %matplotlib inline
#!pip install pydot
import pydot
from IPython.display import Image
from six import StringIO
from sklearn.tree import export_graphviz

"""# Data"""

def csv_drive_path_generatoer(url):
 path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
 return path

# url
url = 'https://drive.google.com/file/d/1GCbkSByQj1H0U1qOw7DvVxF9zRswWP1e/view?usp=sharing'


data_url = csv_drive_path_generatoer(url)
df = pd.read_csv(data_url, header=int(), squeeze=True)

df

"""# EDA"""

print(df.shape)
print(df.info())
print(df.isnull().sum())
print(df.nunique())

# info
df.describe()

sb.countplot(df['Gender'])

df = df.drop(columns=['User_ID'])
df.hist(bins = 15, figsize=(15,15), color = "C0")
plt.show();

# df = pd.concat([exercise,calories['Calories']], axis = 1)
# df = pd.read_csv(data_url, header=int(), squeeze=True)

plot = sb.pairplot( df.iloc[:,0:], size=3, markers=".", hue="Gender", kind="reg" )

# get x labels from the last row and save them into an array
xlabels = []
for ax in plot.axes[ -1, : ]:
    xlabel = ax.xaxis.get_label_text()
    xlabels.append( xlabel )
    
# apply x labels from the array to the rest of the graphs
y_ax_len = len( plot.axes[ :, 0 ] )
for i in range( len( xlabels ) ):
    for j in range( y_ax_len ):
        if j != i :
            plot.axes[ j, i ].xaxis.set_label_text( xlabels[ i ] )

df = pd.get_dummies(df)
df = df.drop(columns=['Gender_male'])

for i, col in enumerate(df.columns):
  if col != 'Calories' and col != 'Gender_female':
    plt.figure(i, figsize=(10,5))
    sb.lineplot(x="Calories", y=col, data=df);
    ax = plt.gca();
    ax.set_title(col.upper());

# calories_exercise = pd.get_dummies(calories_exercise)

plt.figure(figsize=(17,7))
sb.heatmap(df.corr(), cmap="crest", annot=True);

"""# Data Enrichment

"""

# ana
# Replace gender column with a binary value 1,0
# df['sex'] = df['Gender'].replace({'female': 0, 'male': 1})
df['sex'] = df['Gender_female'].replace({'1': 0, '0': 1})

# df = df.drop(columns=['Gender','User_ID'])
df.head()

"""# Linear Regression

"""

df.replace({"Gender":{'male':0,'female':1}}, inplace=True)

X_train, X_test = split(df, test_size=0.3, random_state=12345)

X_train_lr =  X_train.drop(columns = ['Calories','Height','Weight','Age'], axis = 1)
y_train_lr = X_train['Calories']

X_test_lr = X_test.drop(columns = ['Calories','Height','Weight','Age'], axis = 1)
y_test_lr = X_test['Calories']

fitting_lm_lr = LinearRegression().fit(X_train_lr, y_train_lr)
y_train_pred_lr = fitting_lm_lr.predict(X_train_lr)

pd.DataFrame({'y_train': y_train_lr, 'y_train_pred': y_train_pred_lr}).head()

#X_train, X_test, y_train, y_test = split(X, y, test_size=0.3, random_state=42)

#model = LinearRegression()
#model.fit(X_train, y_train)


#formula = 'Calories = ' + f'{model.intercept_:.3f}'

#formula = 'Weight = ' + f'{model.intercept_:.3f}'

#y_pred = model.predict(X_test)





#plt.scatter(X_test, y_test, color='blue') # plotting the observation line
#plt.plot(X_test, y_pred, color='red') # plotting the regression line
#plt.show()

train_rmse_lr = np.sqrt(mean_squared_error(y_train_lr, y_train_pred_lr))
print(f"RMSE (train)= {train_rmse_lr:.2f}")


y_test_pred_lr = fitting_lm_lr.predict(X_test_lr)
test_rmse_lr = np.sqrt(mean_squared_error(y_test_lr, y_test_pred_lr))
print(f"RMSE (test lm) = {test_rmse_lr:.2f}")

plt.figure(figsize=(5,5))
plt.scatter(y_train_lr, y_train_pred_lr, c='crimson')

p1 = max(max(y_train_pred_lr), max(y_train_lr))
p2 = min(min(y_train_pred_lr), min(y_train_lr))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.axis('equal')
plt.show()

plt.figure(figsize=(5,5))
plt.scatter(y_test_lr, y_test_pred_lr, c='crimson')

p1 = max(max(y_test_pred_lr), max(y_test_lr))
p2 = min(min(y_test_pred_lr), min(y_test_lr))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.axis('equal')
plt.show()

"""# Linear Regression using Gradient Descent

"""

from sklearn.linear_model import LinearRegression
import seaborn as sns

#model 1 - Predicting calories based on duration 
X= df['Duration']
Y= df['Calories']
sns.scatterplot(X,Y)

theta0 = 0
theta1 = 0
alpha = 0.000001
count = 15000
m = len(X)

for i in range(count): 
    Y_hat = theta1*X + theta0  
    theta0 = theta0 - (alpha/m)*sum(Y_hat-Y)
    theta1 = theta1 - (alpha/m)*sum(X*(Y_hat-Y))


    
print(theta0,theta1)

Y_hat = theta1*X + theta0

plt.scatter(X, Y) 
plt.plot([min(X), max(X)], [min(Y_hat), max(Y_hat)], color='red')  # regression line
plt.show()

import math
def RSE(y_true, y_predicted):
   
    y_true = np.array(y_true)
    y_predicted = np.array(y_predicted)
    RSS = np.sum(np.square(y_true - y_predicted))

    rse = math.sqrt(RSS / (len(y_true) - 2))
    return rse


rse= RSE(df['Duration'],Y_hat)
print(rse)

from sklearn.metrics import mean_squared_error

# rmse = np.sqrt(mean_squared_error(y_true,y_predicted))

#Using Scikit-Learn
X = np.array(df['Duration']).reshape(-1,1)
y = np.array(df['Calories']).reshape(-1,1)
 

model = LinearRegression()
model.fit(X,y)


print(model.coef_)
print(model.intercept_)

y_predict = model.predict(X)

rmse = np.sqrt(mean_squared_error(y,y_predict))

print(rmse)

#Predicting clalories
Duration = [20]
result = model.predict([Duration])
print(result)

Duration = [35]
result = model.predict([Duration])
print(result)

#model 2 - Predicting calories based on body temp 
X= df['Body_Temp']
Y= df['Calories']
sns.scatterplot(X,Y)

theta0 = 0
theta1 = 0
alpha = 0.000001
count = 15000
m = len(X)

for i in range(count): 
    Y_hat = theta1*X + theta0  
    theta0 = theta0 - (alpha/m)*sum(Y_hat-Y)
    theta1 = theta1 - (alpha/m)*sum(X*(Y_hat-Y))
    
    
print(theta0,theta1)

Y_hat = theta1*X + theta0

plt.scatter(X, Y) 
plt.plot([min(X), max(X)], [min(Y_hat), max(Y_hat)], color='red')  # regression line
plt.show()

import math
def RSE(y_true, y_predicted):
   
    y_true = np.array(y_true)
    y_predicted = np.array(y_predicted)
    RSS = np.sum(np.square(y_true - y_predicted))

    rse = math.sqrt(RSS / (len(y_true) - 2))
    return rse


rse= RSE(df['Body_Temp'],Y_hat)
print(rse)

#Using Scikit-Learn
X = np.array(df['Body_Temp']).reshape(-1,1)
y = np.array(df['Calories']).reshape(-1,1)
 

model = LinearRegression()
model.fit(X,y)


print(model.coef_)
print(model.intercept_)

y_predict = model.predict(X)

rse = RSE(y,y_predict)

print(rse)

rmse = np.sqrt(mean_squared_error(y,y_predict))
print (rmse)

Body_Temp = [40]
result = model.predict([Body_Temp])
print(result)

Body_Temp = [39]
result = model.predict([Body_Temp])
print(result)

#LinearRegression, Ridge, Lasso
from sklearn.linear_model import  LinearRegression, Ridge, Lasso

from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)

def rmse(ytrue, ypredicted):
    return np.sqrt(mean_squared_error(ytrue, ypredicted))

lasso = Lasso(max_iter = 100000, normalize = True)

lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)
lassocv.fit(X_train, y_train)

lasso.set_params(alpha=lassocv.alpha_)
lasso.fit(X_train, y_train)

print('The Lasso I:')
print("Alpha =", lassocv.alpha_)
print("RMSE =", rmse(y_test, lasso.predict(X_test)))

# Let's try the same. This time setting up alpha...
alpha = np.geomspace(1e-5, 1e0, num=6)
lasso_cv_model = LassoCV(alphas = alpha, cv = 10, max_iter = 100000, normalize = True).fit(X_train,y_train)
lasso_tuned = Lasso(max_iter = 100000, normalize = True).set_params(alpha = lasso_cv_model.alpha_).fit(X_train,y_train)
print('The Lasso II:')
print("Alpha =", lasso_cv_model.alpha_)
print("RMSE =", rmse(y_test, lasso_tuned.predict(X_test)))

alphas = np.geomspace(1e-9, 5, num=1000)

ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)
ridgecv.fit(X_train, y_train)

ridge = Ridge(alpha = ridgecv.alpha_, normalize = True)
ridge.fit(X_train, y_train)

print('Ridge Regression:')
print("Alpha =", ridgecv.alpha_)
print("RMSE =", rmse(y_test, ridge.predict(X_test)))

"""# Regression Tree"""

# Divide the dataset into X and y
X = df.drop('Calories', axis=1)
y = df['Calories']

# Divide the dataset into train and test data
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=142857)

# Hyperparameter tuning
# Check what is the best max depth parameter for this model
max_depths = range(1, 20)
training_error = []
for max_depth in max_depths:
    model_1 = DecisionTreeRegressor(max_depth=max_depth)
    model_1.fit(X, y)
    training_error.append(msle(y, model_1.predict(X))**0.5)
    
testing_error = []
for max_depth in max_depths:
    model_2 = DecisionTreeRegressor(max_depth=max_depth)
    model_2.fit(X_train, y_train)
    testing_error.append(msle(y_train, model_2.predict(X_train))**0.5)

plt.plot(max_depths, training_error, color='blue', label='Training error')
plt.plot(max_depths, testing_error, color='green', label='Testing error')
plt.xlabel('Tree depth')
plt.ylabel('RMSE')
plt.title('Hyperparameter Tuning', pad=15, size=15)
plt.legend()

# k-fold cross-validation
from sklearn.model_selection import GridSearchCV

model = DecisionTreeRegressor()

gs = GridSearchCV(model,
                  param_grid = {'max_depth': range(1, 11),
                                'min_samples_split': range(10, 60, 10),
                                'max_leaf_nodes': range(5, 20, 5)},
                  cv=5,
                  n_jobs=1,
                  scoring='neg_mean_squared_error')

gs.fit(X_train, y_train)

print(gs.best_params_)
print(-gs.best_score_)

# We will fit the model with the best hyperparameters
model = DecisionTreeRegressor(max_leaf_nodes=15, max_depth=5, min_samples_split=10)
model.fit(X_train, y_train)

# And we will draw the tree
dot_data = StringIO()  
export_graphviz(model, out_file=dot_data, feature_names=X.columns, leaves_parallel=True)  
graph = pydot.graph_from_dot_data(dot_data.getvalue())[0]  
Image(graph.create_png(), width=1750, height=750)

y_pred = model.predict(X)

# We will then score the model by RMSE
cal_rmse = np.sqrt(msle(y, y_pred))
print(f"RMSE = {cal_rmse:.2f}")

# We can view the importance of the different features in the model
for feature, importance in zip(X.columns, model.feature_importances_):
    print(f'{feature:12}: {importance}')

"""# K Nearest Neighbors """

def rmsle(y_true: pd.Series, y_pred:pd.Series):
  return (((np.log(y_true)-np.log(y_pred))**2).mean())**0.5


def r_squared(y_true, y_pred):
    y_bar = y.mean()
    ss_tot = ((y_true - y_bar)**2).sum()
    ss_res = ((y_true - y_pred)**2).sum()
    return 1 - (ss_res/ss_tot)

# X = all fields without Height
X = df.drop(['Calories', 'Height'], axis=1)

y = df['Calories']

k = 9


model = KNeighborsRegressor(n_neighbors=k)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)

# Train the model using the training sets
model.fit(X_train, y_train)

#Predict Output
y_test_pred = model.predict(X_test)

y_train_pred = model.predict(X_train)

# 
rmsle_test = rmsle(y_test, y_test_pred)
rmsle_train = rmsle(y_train, y_train_pred)

rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))

r_squared_test = r_squared(y_test, y_test_pred)
r_squared_train = r_squared(y_train, y_train_pred)
print(X.head())
print('')

print(f"k = {k}")
print('')

print(f"rmse test = {rmse_test:.2f}")
print(f"rmse train = {rmse_train:.2f}")
print('')

round_df = pd.DataFrame({'y test': y_test, 'y test pred': y_test_pred})
pd.options.display.float_format = "{:,.2f}".format
round_df

knn_n =[]
rmse_test = []
rmse_train = []
for i in range(1,16):
    model = KNeighborsRegressor(n_neighbors=i)
    model.fit(X_train,y_train)
    
    y_test_pred = model.predict(X_test)
    y_train_pred = model.predict(X_train)

    # rmse
    rmse_test.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))
    rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))

    knn_n.append(i)
plt.figure(figsize=[10,5])
plt.plot(knn_n, rmse_test, label = 'RMSE Test')
plt.plot(knn_n, rmse_train, label = 'RMSE Train')
plt.legend()
plt.title('Number of Neighbours')
plt.xlabel('Neighbors')
plt.ylabel('RMSE')
plt.xticks(knn_n)
plt.show()